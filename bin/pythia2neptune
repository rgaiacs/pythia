#!/usr/bin/env python
import argparse
import io
import logging
import time

import pandas
import neptune

from pythia import svm
from pythia import util

logging.basicConfig(
    level=logging.NOTSET,
    format='%(message)s'
)
logger = logging.getLogger(__name__)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='CRIC SVM to Neptune.'
    )
    parser.add_argument(
        '--verbose',
        '-v',
        action='store_true',
        help='Enable verbose mode'
    )
    parser.add_argument(
        '--user',
        default="CRICDatabase",
        help='User name in Neptune'
    )
    parser.add_argument(
        '--project',
        default="pythia",
        help='Project name in Neptune'
    )
    parser.add_argument(
        '--experiment',
        default="example",
        help='Experiment name in Neptune'
    )
    parser.add_argument(
        '--image-folder',
        default="img",
        help='Image folder'
    )
    parser.add_argument(
        '--section-size',
        type=int,
        default=100,
        help='Image section size'
    )

    args = parser.parse_args()

    if args.verbose:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.WARNING)

    neptune.init(
        "{}/{}".format(
            args.user,
            args.project
        )
    )

    neptune.create_experiment(
        name=args.experiment
    )

    TRAIN_TEST_FILENAMES = [
        (
            "train-1.json",
            "test-1.json",
        )
    ]

    for train_filename, test_filename in TRAIN_TEST_FILENAMES:
        logger.info('Fitting {} ...'.format(train_filename))
        t0 = time.time()
        svc = svm.SVC(
            train_filename,
            args.image_folder,
            args.section_size,
            crop_grid=True,
            crop_center=True
        )
        operation_time = time.time() - t0
        neptune.log_metric('Training time', operation_time)
        logger.info('Fitting {} complete!'.format(train_filename))


        logger.info('Predict {} ...'.format(test_filename))

        predictions = []
        classifications = []
        i = 0
        operation_time = 0
        for section, classification in util.collection2sections_and_classes(
                test_filename,
                args.image_folder,
                args.section_size):

            t0 = time.time()
            predictions.append(svc.predict_sample(
                section
            ))
            operation_time = operation_time + time.time() - t0

            i = i + 1
            classifications.append(classification)

        operation_time = operation_time / i
        neptune.log_metric('(Average) Prediction time', operation_time)

        logger.info('Predict {} complete!'.format(test_filename))

        df = pandas.DataFrame({
            "manual": classifications,
            "prediction": predictions
        })
        
        neptune.log_metric(
            'Number of cells',
            df.shape[0]
        )

        true_positive = df[
            (df["manual"] == "altered cell") & (df["prediction"] == "altered cell")
        ].shape[0]
        neptune.log_metric(
            'True Positive',
            true_positive
        )
        
        true_negative = df[
            (df["manual"] == "normal cell") & (df["prediction"] == "normal cell")
        ].shape[0]
        neptune.log_metric(
            'True Negative',
            true_negative
        )

        false_positive = df[
            (df["manual"] == "normal cell") & (df["prediction"] == "altered cell")
        ].shape[0]
        neptune.log_metric(
            'False Positive',
            false_positive
        )
        
        false_negative = df[
            (df["manual"] == "altered cell") & (df["prediction"] == "normal cell")
        ].shape[0]
        neptune.log_metric(
            'False Negative',
            false_negative
        )

        precision = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
        neptune.log_metric(
            'Precision',
            precision
        )

        sensitivity = (true_positive) / (true_positive + false_negative)
        neptune.log_metric(
            'Sesitivity',
            sensitivity
        )

        specificity = (true_negative) / (true_negative + false_positive)
        neptune.log_metric(
            'Specificity',
            specificity
        )

        f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)
        neptune.log_metric(
            'F1 score',
            f1_score
        )

        neptune.log_artifact(
            io.StringIO(df.to_csv()),
            "prediction_log.csv"
        )

    neptune.stop()
